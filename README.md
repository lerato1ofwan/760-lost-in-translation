🌍 Machine Translation Evaluation Using the Corrected FLORES Datasets
Group 19 – COS760 UP Natural Language Processing 2025

🏃 How to run:

-Clone the github repository.

-Execute "Lost_In_Translation_Notebook.ipynb" notebook file: Upload and run all cells either in Google Colab T4 runtime, or locally in Jupyter Notebook or equivalent.

-We strongly advice using Google Colab's T4 runtime if local Jupyter Notebook is not powered by a GPU - the NLLB 3B model requires a GPU for faster processing.

📌 Project Overview

This project evaluates the quality of machine translation (MT) systems before and after the correction of the FLORES dataset for four low-resource African languages: Hausa, Sepedi, Xitsonga, and isiZulu. Using both automated evaluation metrics and explainability techniques, we identify improvements in translation quality and diagnose common translation errors. The ultimate goal is to validate and strengthen dataset integrity for better MT outcomes.

🧠 Key Objectives

-Assess translation performance before vs. after dataset corrections.

-Apply and compare state-of-the-art evaluation metrics: BLEU, chrF++, COMET, BLEURT.

-Use explainability methods (e.g. SHAP, error spans) to explore translation errors.

-Investigate dataset validation strategies for low-resource language corpora.

🧪 Dataset

-We use the Corrected FLORES Dataset provided by the Masakhane community:

-Languages: Hausa, Sepedi, Xitsonga, isiZulu

-Task: Evaluate translations generated by NLLB models using both the original and corrected test sets.

📊 Evaluation Metrics

We use both automatic metrics and human-aligned scoring systems to assess quality:

Metric	Type	Description : 
BLEU	Lexical / N-gram	Measures n-gram precision vs. reference translations. Common in MT benchmarking.
chrF++	Character-level	Captures word morphology better; more reliable for morphologically rich languages.
COMET	Neural / Semantic	Learned metric aligned to human judgments. Considers source and hypothesis context.
BLEURT	Pretrained / BERT-based	Fine-tuned for semantic similarity and adequacy, even with lexical variation.

⚖️ We observed that COMET and BLEURT correlated better with human judgments, especially post-correction.

🔎 Explainability & Error Analysis:

-To go beyond numbers, we analyzed translations using:

-Error highlighting and span-level visualizations

-LIME and SHAP for model decision transparency

-Patterns of hallucinations, omissions, and mistranslations

Findings:
-Corrected datasets led to a notable drop in hallucinated content

-More consistent outputs with respect to tense, named entities, and number agreement

-Explainability methods revealed persistent error patterns in Sepedi and isiZulu

✅ Dataset Validation Strategies:

-We explored ways to validate translations:

-Back-translation comparison

-Consensus voting from multiple annotators

-Target-side language model scoring

-Use of contrastive test cases

🛠️ Tech Stack : 

-Python 3.10+

-HuggingFace Transformers

-sacreBLEU, COMET, BLEURT

-Lightweight tools for explainability

-Google Colab & PyTorch

📈 Results Summary:

-chrF++ and COMET showed the largest improvements post-correction (~3-5 point increase)

-Zulu and Sepedi showed the most variation in quality, suggesting more noise in the original datasets

-Human spot-checking aligned best with BLEURT and COMET scores

👥 Team Members:

Member 1 – Mishka Dukhanti

Member 2 – Lerato Letsepe

Member 3 – Aidan Govender

📄 Citation & Acknowledgements
We thank the Masakhane community for their work on FLORES corrections and Meta AI for the NLLB models.

📚 References

Papineni et al., “BLEU: a Method for Automatic Evaluation of Machine Translation”, ACL 2002

Rei et al., “COMET: A Neural Framework for MT Evaluation”, EMNLP 2020

Sellam et al., “BLEURT: Learning Robust Metrics for Text Generation”, ACL 2020

Mielke et al., “Between BLEU and BERT: A Survey of Automatic Metrics for NLP Evaluation”, ACL 2021
