{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5FJwpIoEb6Dy",
        "outputId": "e98c4820-2d44-48ae-c357-0f894eda6648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: hf_xet in /usr/local/lib/python3.11/dist-packages (1.1.3)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: comet-ml in /usr/local/lib/python3.11/dist-packages (3.49.11)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (3.2.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (0.22.8)\n",
            "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (3.1.0)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (4.24.0)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (5.9.5)\n",
            "Requirement already satisfied: python-box<7.0.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (6.1.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (1.0.0)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (13.9.4)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (2.29.1)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.11/dist-packages (from comet-ml) (3.20.1)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (1.17.2)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from comet-ml) (3.1.1)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.11/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet-ml) (5.0.9)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml) (0.25.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet-ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.2->comet-ml) (2.19.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet-ml) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n",
            "fatal: destination path 'bleurt' already exists and is not an empty directory.\n",
            "/content/bleurt\n",
            "Processing /content/bleurt\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.15.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (2.18.0)\n",
            "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2025.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (4.25.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.72.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.1.2)\n",
            "Building wheels for collected packages: BLEURT\n",
            "  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16456766 sha256=737d7132575fbf9707f2be225bde6d2411325542b7945f49813925062a0e3b66\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l88tqv5c/wheels/49/ab/73/9318ab38d4cd1c732bcea8335d3f8d7c0316c8d07b9084fa85\n",
            "Successfully built BLEURT\n",
            "Installing collected packages: BLEURT\n",
            "  Attempting uninstall: BLEURT\n",
            "    Found existing installation: BLEURT 0.0.2\n",
            "    Uninstalling BLEURT-0.0.2:\n",
            "      Successfully uninstalled BLEURT-0.0.2\n",
            "Successfully installed BLEURT-0.0.2\n",
            "/content\n",
            "Collecting git+https://github.com/Unbabel/COMET\n",
            "  Cloning https://github.com/Unbabel/COMET to /tmp/pip-req-build-llmxj8ve\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Unbabel/COMET /tmp/pip-req-build-llmxj8ve\n",
            "  Resolved https://github.com/Unbabel/COMET to commit 49c348dd0ff569c4d7c3f2b7c720fd5696f8de59\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: entmax<2.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (1.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (0.32.4)\n",
            "Requirement already satisfied: jsonargparse==3.13.1 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (3.13.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (2.2.2)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.24.4 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (4.25.8)\n",
            "Requirement already satisfied: pytorch-lightning<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (2.5.1.post0)\n",
            "Requirement already satisfied: sacrebleu<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (2.5.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (1.15.3)\n",
            "Requirement already satisfied: sentencepiece<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchmetrics<0.11.0,>=0.10.2 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (0.10.3)\n",
            "Requirement already satisfied: transformers<5.0,>=4.17 in /usr/local/lib/python3.11/dist-packages (from unbabel-comet==2.2.6) (4.52.4)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.11/dist-packages (from jsonargparse==3.13.1->unbabel-comet==2.2.6) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (1.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.1->unbabel-comet==2.2.6) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.1->unbabel-comet==2.2.6) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.1->unbabel-comet==2.2.6) (2025.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (0.14.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (3.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (5.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->unbabel-comet==2.2.6) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->unbabel-comet==2.2.6) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0,>=4.17->unbabel-comet==2.2.6) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0,>=4.17->unbabel-comet==2.2.6) (0.5.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet==2.2.6) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->unbabel-comet==2.2.6) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet==2.2.6) (2025.4.26)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet==2.2.6) (1.20.0)\n"
          ]
        }
      ],
      "source": [
        "# Install all necessary NLP and evaluation libraries\n",
        "!pip install -U transformers datasets hf_xet sacrebleu evaluate bert-score comet-ml\n",
        "\n",
        "# BLEURT: clone and install from source (preferred method)\n",
        "!git clone https://github.com/google-research/bleurt.git\n",
        "%cd bleurt\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "!pip install git+https://github.com/Unbabel/COMET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_OY9cFLLxYWw"
      },
      "outputs": [],
      "source": [
        "# Project Imports\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "H79yM9iycbV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad06b936-918f-405a-e580-24d5dbbdf1d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading FLORES-200 English 'dev' split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 997 English sentences from FLORES-200 'dev' split.\n",
            "\n",
            "First 5 English source sentences (DEV):\n",
            "1. On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.\n",
            "2. Lead researchers say this may bring early detection of cancer, tuberculosis, HIV and malaria to patients in low-income countries, where the survival rates for illnesses such as breast cancer can be half those of richer countries.\n",
            "3. The JAS 39C Gripen crashed onto a runway at around 9:30 am local time (0230 UTC) and exploded, closing the airport to commercial flights.\n",
            "4. The pilot was identified as Squadron Leader Dilokrit Pattavee.\n",
            "5. Local media reports an airport fire vehicle rolled over while responding.\n",
            "Loaded 997 original Sepedi sentences for 'dev' split.\n",
            "\n",
            "First 5 Original Sepedi references (DEV):\n",
            "1. Ka Mošupulogo, boramahlale ba go tšwa Sekolong sa Yunibesithi ya Stanford sa Medicine ba tsebišitše ka go dirwa ga sedirišwa se seswa sa tekolo seo se ka beakanyago disele ka mehuta: chip yeo e gatišegago ye nnyane yeo e ka dirwago go šomišwa di printer tša inkjet tša sente ye tee go U.S.\n",
            "2. Banyakišiši ba ketapele ba re se se ka tliša temogo ya kankere, tuberculosis, HIV le malaria go balwetši kapela dinageng tša letseno la fase, moo ditekano tša go phela tša malwetši a bjalo ka kankere ya letswele di ka bago seripa sa tša dinaga tšeo di humilego.\n",
            "3. JAS 39C Gripen e wetše godimo ga moo sefofane se sepelago gona ka bo 9:30 am nako ya selegae (0230UTC) gomme ya thuthupa, gwa tswalelwa boemafofane bakeng sa difofane tša tefelo.\n",
            "4. Mootledi wa sefofane o tsebišitšwe bjalo ka Moetapele Dilokrit Pattavee.\n",
            "5. Diphatlalatšo ya ditaba tša selegae di bega gore senamelwa sa moolo sa boemafofane se tokologile ge se bitšwa.\n",
            "\n",
            "Loading FLORES-200 English 'devtest' split...\n",
            "Loaded 1012 English sentences from FLORES-200 'devtest' split.\n",
            "\n",
            "First 5 English source sentences (DEVTEST):\n",
            "1. \"We now have 4-month-old mice that are non-diabetic that used to be diabetic,\" he added.\n",
            "2. Dr. Ehud Ur, professor of medicine at Dalhousie University in Halifax, Nova Scotia and chair of the clinical and scientific division of the Canadian Diabetes Association cautioned that the research is still in its early days.\n",
            "3. Like some other experts, he is skeptical about whether diabetes can be cured, noting that these findings have no relevance to people who already have Type 1 diabetes.\n",
            "4. On Monday, Sara Danius, permanent secretary of the Nobel Committee for Literature at the Swedish Academy, publicly announced during a radio program on Sveriges Radio in Sweden the committee, unable to reach Bob Dylan directly about winning the 2016 Nobel Prize in Literature, had abandoned its efforts to reach him.\n",
            "5. Danius said, \"Right now we are doing nothing. I have called and sent emails to his closest collaborator and received very friendly replies. For now, that is certainly enough.\"\n",
            "Loaded 1012 original Sepedi sentences for 'devtest' split.\n",
            "\n",
            "First 5 Original Sepedi references (DEVTEST):\n",
            "1. \"Re na gabjale le magotlwana a kgwedi-tše-4 ao a se nago bolwetši bja sukiri ao a bego a na le bolwetši bja sukiri,\" o okeditše.\n",
            "2. Dr. Ehud Ur, moprofesara wa tša maphelo Yunibesithing ya Dalhousie ka Halifax, Nova Scotia le modulasetulo wa karolo ya maphelo le mahlale wa Mokgahlo wa Bolwetši bja Swikiri wa Canada ba lemošitše gore nyakišišo e sa le matšatšing a pele.\n",
            "3. Bjalo ka ditsebi tše dingwe, o gononwa gore ekaba diabetes e ka fodišwa, a bolela gore dikarabo tše ga di bolele ka batho bao ba šetšego ba na le Type 1 diabetes.\n",
            "4. Ka Mošupulogo, Sara Danius, mongwaledi wa goya goile wa Komiti ye Kgethwa ya Dipuku ka Akhatemi ya Swedish, o tsebišitše seyalemoyeng phatlatlatša go Seyalemoya sa Sveriges ka Sweden, a palelwa ke go fihlelela Bob Dylan thwii mabapi le go thopa Sefoka sa 2016 ka Dipuku, o tlogetše maitapišo a yona go mo fihlelela.\n",
            "5. Danius o boletše gore, \"Gabjale ga re dire selo. Ke leditše le go romela diimeile go modirišanimmogo le yena gomme ka amogela diphetolo tše dibotse. Gabjale, seo se lekane.\"\n"
          ]
        }
      ],
      "source": [
        "# Using datasets library to load 'dev' English source from Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Loading the FLORES-200 'dev' split, focusing on English (eng_Latn-nso_Latn)\n",
        "# This configuration ensures parallel sentences across English and Sepedi languages.\n",
        "# We're loading the English text we will use to input into our model as the source,\n",
        "# and the Sepedi (nso_Latn) first reference\n",
        "print(\"Loading FLORES-200 English 'dev' split...\")\n",
        "flores_dev = load_dataset(\"facebook/flores\", \"eng_Latn-nso_Latn\", split=\"dev\", trust_remote_code=True)\n",
        "print(f\"Loaded {len(flores_dev)} English sentences from FLORES-200 'dev' split.\")\n",
        "\n",
        "# Extracting just the English sentences for translation\n",
        "# We'll use this list as input to our MT models\n",
        "english_source_sentences_dev = [item['sentence_eng_Latn'] for item in flores_dev]\n",
        "\n",
        "print(\"\\nFirst 5 English source sentences (DEV):\")\n",
        "for i, sentence in enumerate(english_source_sentences_dev[:5]):\n",
        "    print(f\"{i+1}. {sentence}\")\n",
        "\n",
        "original_sepedi_references_dev = [item['sentence_nso_Latn'] for item in flores_dev]\n",
        "print(f\"Loaded {len(original_sepedi_references_dev)} original Sepedi sentences for 'dev' split.\")\n",
        "\n",
        "print(\"\\nFirst 5 Original Sepedi references (DEV):\")\n",
        "for i, sentence in enumerate(original_sepedi_references_dev[:5]):\n",
        "    print(f\"{i+1}. {sentence}\")\n",
        "\n",
        "# This will be for the final 'devtest' split for comprehensive evaluation later.\n",
        "# Continuing tests with focus on 'dev' for now.\n",
        "print(\"\\nLoading FLORES-200 English 'devtest' split...\")\n",
        "flores_devtest = load_dataset(\"facebook/flores\", \"eng_Latn-nso_Latn\", split=\"devtest\", trust_remote_code=True)\n",
        "print(f\"Loaded {len(flores_devtest)} English sentences from FLORES-200 'devtest' split.\")\n",
        "english_source_sentences_devtest = [item['sentence_eng_Latn'] for item in flores_devtest]\n",
        "\n",
        "print(\"\\nFirst 5 English source sentences (DEVTEST):\")\n",
        "for i, sentence in enumerate(english_source_sentences_devtest[:5]):\n",
        "    print(f\"{i+1}. {sentence}\")\n",
        "\n",
        "original_sepedi_references_devtest = [item['sentence_nso_Latn'] for item in flores_devtest]\n",
        "print(f\"Loaded {len(original_sepedi_references_devtest)} original Sepedi sentences for 'devtest' split.\")\n",
        "\n",
        "print(\"\\nFirst 5 Original Sepedi references (DEVTEST):\")\n",
        "for i, sentence in enumerate(original_sepedi_references_devtest[:5]):\n",
        "   print(f\"{i+1}. {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar8sqOUEt38q"
      },
      "source": [
        "### Load corrected datasets from dfsi Github repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "jk3vCoL8t31z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ae34c3-01c4-4798-c4d4-889a41348c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning GitHub repository: https://github.com/dsfsi/flores-fix-4-africa.git\n",
            "Repository 'flores-fix-4-africa' already cloned.\n",
            "Corrected data base path set to: /content/flores-fix-4-africa/data/corrected\n"
          ]
        }
      ],
      "source": [
        "# Clone repo\n",
        "github_repo_url = \"https://github.com/dsfsi/flores-fix-4-africa.git\"\n",
        "repo_name = github_repo_url.split('/')[-1].replace('.git', '') # Extracts 'flores-fix-4-africa'\n",
        "print(f\"Cloning GitHub repository: {github_repo_url}\")\n",
        "# Check if the directory already exists to avoid cloning multiple times on reruns\n",
        "if not os.path.exists(f\"/content/{repo_name}\"):\n",
        "    !git clone {github_repo_url}\n",
        "    print(f\"Repository '{repo_name}' cloned successfully.\")\n",
        "else:\n",
        "    print(f\"Repository '{repo_name}' already cloned.\")\n",
        "\n",
        "# Base path to the corrected data within the cloned repository\n",
        "# The structure is: repo_name/data/corrected/{split_type}/{lang_code}.{split_type}\n",
        "base_corrected_data_path = os.path.join(\"/content\", repo_name, \"data\", \"corrected\")\n",
        "print(f\"Corrected data base path set to: {base_corrected_data_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RYmEaT2huW-G"
      },
      "outputs": [],
      "source": [
        "def load_corrected_language_data(lang_code: str, split_type: str) -> list:\n",
        "    \"\"\"\n",
        "    Loads corrected reference sentences for a given language and split type\n",
        "    from the cloned flores-fix-4-africa GitHub repository.\n",
        "\n",
        "    Args:\n",
        "        lang_code (str): The language code (e.g., \"nso_Latn\", \"hau_Latn\").\n",
        "        split_type (str): The split type (\"dev\" or \"devtest\").\n",
        "\n",
        "    Returns:\n",
        "        list: A list of corrected sentences. Returns an empty list if the file is not found.\n",
        "    \"\"\"\n",
        "    # Construct the full file path dynamically\n",
        "    file_path = os.path.join(base_corrected_data_path, split_type, f\"{lang_code}.{split_type}\")\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            sentences = [line.strip() for line in f if line.strip()]\n",
        "        print(f\"Loaded {len(sentences)} corrected '{lang_code}' sentences for '{split_type}'.\")\n",
        "        return sentences\n",
        "    else:\n",
        "        print(f\"WARNING: Corrected '{lang_code}' '{split_type}' file not found at {file_path}. Skipping.\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XKg3IBRFugCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12517ae-f6ee-4fc5-e304-6eca0991594f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 997 corrected 'nso_Latn' sentences for 'dev'.\n",
            "Loaded 1012 corrected 'nso_Latn' sentences for 'devtest'.\n",
            "\n",
            "Example of accessing Sepedi (nso_Latn) dev corrected sentences:\n",
            "['Ka Mošupulogo, boramahlale ba go tšwa Sekolong sa Yunibesithi ya Stanford sa tša maphelo ba tsebišitše ka go hlangwa ga sedirišwa se sefsa sa tekolo seo se ka beakanyago disele ka mehuta: chip yeo e gatišegago ye nnyane yeo e ka tšweletšwago ka go šomiša metšhene ya go gatiša ya inkjet ya sente ye tee ya U.S. ka o tee.', 'Banyakišiši ba ketapele ba re se se ka tliša temogo ya kankere, tuberculosis, HIV le malaria go balwetši kapela dinageng tša letseno la fase, moo ditekano tša go phela tša malwetši a bjalo ka kankere ya letswele di ka bago seripa sa tša dinaga tšeo di humilego.', 'JAS 39C Gripen e wetše godimo ga moo sefofane se sepelago gona ka bo 9:30 am nako ya selegae (0230UTC) gomme ya thuthupa, gwa tswalelwa boemafofane bakeng sa difofane tša tefelo.', 'Mootledi wa sefofane o tsebišitšwe bjalo ka Moetapele Dilokrit Pattavee.', 'Diphatlalatšo ya ditaba tša selegae di bega gore senamelwa sa moolo sa boemafofane se tokologile ge se bitšwa.']\n"
          ]
        }
      ],
      "source": [
        "# Loading the corrected data fro the languages\n",
        "all_corrected_references_by_lang = {\n",
        "    \"dev\": {},\n",
        "    \"devtest\": {}\n",
        "}\n",
        "\n",
        "# All all African languages with corrected data\n",
        "african_languages_corrected = [\"nso_Latn\"] #  [\"hau_Latn\", \"nso_Latn\", \"tso_Latn\", \"zul_Latn\"]\n",
        "\n",
        "for lang_code in african_languages_corrected:\n",
        "    all_corrected_references_by_lang[\"dev\"][lang_code] = load_corrected_language_data(lang_code, \"dev\")\n",
        "    all_corrected_references_by_lang[\"devtest\"][lang_code] = load_corrected_language_data(lang_code, \"devtest\")\n",
        "\n",
        "# Example of accessing the data:\n",
        "print(\"\\nExample of accessing Sepedi (nso_Latn) dev corrected sentences:\")\n",
        "print(all_corrected_references_by_lang[\"dev\"][\"nso_Latn\"][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we_feZudiEH7"
      },
      "source": [
        "## Load NLLB-200 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ip2av260iTv5"
      },
      "outputs": [],
      "source": [
        "# Defining the NLLB-200 model name, and methods to translate and save translations\n",
        "\n",
        "# nllb_model_name = \"facebook/nllb-200-distilled-600M\" # lightweight version distilled-600M\n",
        "nllb_model_name = \"facebook/nllb-200-1.3B\" # Larger model, may have better quality of results\n",
        "\n",
        "\n",
        "def save_translations_to_file(\n",
        "    translated_texts: list,\n",
        "    output_folder: str,\n",
        "    filename_prefix: str,\n",
        "    src_lang: str,\n",
        "    tgt_lang: str,\n",
        "    split_type: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Saves a list of translated texts to a text file.\n",
        "\n",
        "    Args:\n",
        "        translated_texts (list): List of strings, where each string is a translated sentence.\n",
        "        output_folder (str): The directory where the output file will be saved.\n",
        "        filename_prefix (str): Prefix for the filename (e.g., \"nllb200\").\n",
        "        src_lang (str): Source language code.\n",
        "        tgt_lang (str): Target language code.\n",
        "        split_type (str): Type of split (e.g., \"dev\", \"devtest\").\n",
        "    \"\"\"\n",
        "    output_dir = output_folder\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"\\nSaving model outputs to {output_dir}\")\n",
        "\n",
        "    # Construct the filename dynamically\n",
        "    output_filename = f\"{filename_prefix}_{src_lang}_to_{tgt_lang}_{split_type}.txt\"\n",
        "    full_output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "    try:\n",
        "        with open(full_output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for sentence in translated_texts:\n",
        "                f.write(sentence + \"\\n\")\n",
        "        print(f\"Saved translations to: {full_output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving translations to {full_output_path}: {e}\")\n",
        "\n",
        "\n",
        "def run_nllb_translation_and_save(\n",
        "    target_lang_code: str,\n",
        "    split_type: str,\n",
        "    english_source_sentences: list,\n",
        "    model_name: str = nllb_model_name,\n",
        "    output_folder: str = \"nllb_generated_translations\",\n",
        "    source_lang_code: str = \"eng_Latn\",\n",
        "    device: int = 0 if torch.cuda.is_available() else -1 # 0 for GPU, -1 for CPU\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Loads NLLB-200 model, performs translation for a given target language and split,\n",
        "    and saves the generated translations to a file.\n",
        "\n",
        "    Args:\n",
        "        target_lang_code (str): The FLORES language code for the target language (e.g., \"nso_Latn\").\n",
        "        split_type (str): The split type (\"dev\" or \"devtest\").\n",
        "        english_source_sentences (list): List of English sentences to translate for this split.\n",
        "        model_name (str): Name of the NLLB model to use.\n",
        "        output_folder (str): Directory to save the output files.\n",
        "        source_lang_code (str): Source language code, defaults to \"eng_Latn\".\n",
        "\n",
        "    Returns:\n",
        "        list: A list of the generated translated texts. Returns an empty list on error.\n",
        "    \"\"\"\n",
        "\n",
        "    selected_device_info = \"GPU\" if device == 0 else \"CPU\"\n",
        "    print(f\"\\n--- Running NLLB-200 Translation for {source_lang_code} to {target_lang_code} ({split_type.upper()} set) ---\")\n",
        "    print(f\"Using device: {selected_device_info}\")\n",
        "\n",
        "    try:\n",
        "        # Initialize the pipeline for the specific language pair\n",
        "        nllb_translator = pipeline(\n",
        "            'translation',\n",
        "            model=model_name,\n",
        "            src_lang=source_lang_code,\n",
        "            tgt_lang=target_lang_code,\n",
        "            device=device # Use GPU if available (0 is usually the first GPU)\n",
        "        )\n",
        "        print(f\"NLLB-200 model '{model_name}' loaded successfully for {source_lang_code} to {target_lang_code}.\")\n",
        "\n",
        "        print(f\"Generating translations for {target_lang_code} ({split_type} set)... This might take a few minutes for 1000 sentences.\")\n",
        "        nllb_translations_raw = nllb_translator(english_source_sentences)\n",
        "        nllb_translated_texts = [item['translation_text'] for item in nllb_translations_raw]\n",
        "        print(f\"Generated {len(nllb_translated_texts)} translations for {target_lang_code} ({split_type} set).\")\n",
        "\n",
        "        print(f\"\\nFirst 5 NLLB-200 generated translations for {target_lang_code} ({split_type.upper()} set):\")\n",
        "        for i, translation in enumerate(nllb_translated_texts[:5]):\n",
        "          print(f\"{i+1}. {translation}\")\n",
        "\n",
        "        # Save the generated translations\n",
        "        save_translations_to_file(\n",
        "            translated_texts=nllb_translated_texts,\n",
        "            output_folder=output_folder,\n",
        "            filename_prefix=\"nllb200\",\n",
        "            src_lang=source_lang_code,\n",
        "            tgt_lang=target_lang_code,\n",
        "            split_type=split_type\n",
        "        )\n",
        "\n",
        "        return nllb_translated_texts\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during NLLB-200 translation or saving for {target_lang_code} ({split_type} set): {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdJvGF0iZJJC"
      },
      "source": [
        "## Translating + Saving model translation outputs for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RG__Z_sZZPuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a36b84e-1896-476e-e060-30e5d6d2642b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning GitHub repository: https://github.com/lerato1ofwan/760-lost-in-translation.git\n",
            "Repository '760-lost-in-translation' already cloned.\n",
            "MT Outputs data base path set to: /content/760-lost-in-translation/data/mt_outputs\n",
            "Loaded 997 dev translations\n",
            "Loaded 1012 devtest translations\n"
          ]
        }
      ],
      "source": [
        "# Save time load our outputs from our github repo, or comment out lines after \"# Run MT\" comment:\n",
        "\n",
        "# Load translations from File / Model outputs\n",
        "\n",
        "import os\n",
        "\n",
        "# GitHub repo details\n",
        "lost_in_translations_github_repo_url = \"https://github.com/lerato1ofwan/760-lost-in-translation.git\"\n",
        "lost_in_translations_repo_name = lost_in_translations_github_repo_url.split('/')[-1].replace('.git', '')  # \"760-lost-in-translation\"\n",
        "repo_path = f\"/content/{lost_in_translations_repo_name}\"\n",
        "\n",
        "# Clone the repo (if not already cloned)\n",
        "print(f\"Cloning GitHub repository: {lost_in_translations_github_repo_url}\")\n",
        "if not os.path.exists(repo_path):\n",
        "    !git clone {lost_in_translations_github_repo_url} {repo_path}\n",
        "    print(f\"Repository '{lost_in_translations_repo_name}' cloned successfully.\")\n",
        "else:\n",
        "    print(f\"Repository '{lost_in_translations_repo_name}' already cloned.\")\n",
        "\n",
        "# File loading helper\n",
        "def load_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Construct full path to mt_outputs directory\n",
        "mt_outputs_path = os.path.join(repo_path, \"data\", \"mt_outputs\")\n",
        "print(f\"MT Outputs data base path set to: {mt_outputs_path}\")\n",
        "\n",
        "# Load model output files\n",
        "dev_outputs = load_file(os.path.join(mt_outputs_path, \"3B_nllb200_eng_Latn_to_nso_Latn_dev.txt\"))\n",
        "devtest_outputs = load_file(os.path.join(mt_outputs_path, \"3B_nllb200_eng_Latn_to_nso_Latn_devtest.txt\"))\n",
        "\n",
        "# Print loaded sizes\n",
        "print(f\"Loaded {len(dev_outputs)} dev translations\")\n",
        "print(f\"Loaded {len(devtest_outputs)} devtest translations\")\n",
        "\n",
        "\n",
        "# Run MT\n",
        "\n",
        "#  # --- Translations and Saving the outputs for later Evaluations: ---\n",
        "# # We first define the target languages we will use, the dev and devtest translations\n",
        "# # For each target language we invoke the function -> \"run_nllb_translation_and_save\" for dev and devtest\n",
        "\n",
        "# african_target_languages = [\"nso_Latn\"] # [\"hau_Latn\", \"nso_Latn\", \"tso_Latn\", \"zul_Latn\"] # All 4 languages\n",
        "\n",
        "# # You might want to store all results in a dictionary for easy access\n",
        "# all_nllb_translations = {\n",
        "#     \"dev\": {},\n",
        "#     \"devtest\": {}\n",
        "# }\n",
        "\n",
        "# for target_lang in african_target_languages:\n",
        "#     print(f\"\\n--- Processing {target_lang} ---\")\n",
        "\n",
        "#     # For FLORES, the English source sentences usually remain the same across different target evaluations.\n",
        "#     # So, english_source_sentences_dev and english_source_sentences_devtest can be reused.\n",
        "\n",
        "#     dev_translations = run_nllb_translation_and_save(\n",
        "#         target_lang_code=target_lang,\n",
        "#         split_type=\"dev\",\n",
        "#         english_source_sentences=english_source_sentences_dev, # Use the global DEV English sentences\n",
        "#         output_folder=\"nllb_generated_translations_distilled\"\n",
        "#     )\n",
        "#     all_nllb_translations[\"dev\"][target_lang] = dev_translations\n",
        "\n",
        "#     devtest_translations = run_nllb_translation_and_save(\n",
        "#         target_lang_code=target_lang,\n",
        "#         split_type=\"devtest\",\n",
        "#         english_source_sentences=english_source_sentences_devtest,  # Use the global DEVTEST English sentences\n",
        "#         output_folder=\"nllb_generated_translations_distilled\"\n",
        "#     )\n",
        "#     all_nllb_translations[\"devtest\"][target_lang] = devtest_translations\n",
        "\n",
        "# print(\"\\n--- All NLLB-200 translations generated and saved for all specified languages. ---\")\n",
        "\n",
        "# # Final summarative prints\n",
        "# print(\"\\n--- Verification of a sample generated translation (DEV Sepedi) ---\")\n",
        "# if \"nso_Latn\" in all_nllb_translations[\"dev\"]:\n",
        "#     print(\"First 5 generated Sepedi (nso_Latn) sentences (DEV):\")\n",
        "#     for i, sentence in enumerate(all_nllb_translations[\"dev\"][\"nso_Latn\"][:5]):\n",
        "#         print(f\"{i+1}. {sentence}\")\n",
        "# else:\n",
        "#     print(\"Sepedi (nso_Latn) DEV translations not found in all_nllb_translations dictionary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6CVqS-7ZKvq"
      },
      "source": [
        "## Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5dC690BcaFkQ"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import CHRF, BLEU\n",
        "bleu_metric = BLEU()\n",
        "chrf_metric = CHRF()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PmLnI_U3ZNbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba26790-619c-4803-f61e-9d742eea6c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Evaluating: Dev\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Baseline not Found for bert-base-multilingual-cased on nso at /usr/local/lib/python3.11/dist-packages/bert_score/rescale_baseline/nso/bert-base-multilingual-cased.tsv\n",
            "Warning: Baseline not Found for bert-base-multilingual-cased on nso at /usr/local/lib/python3.11/dist-packages/bert_score/rescale_baseline/nso/bert-base-multilingual-cased.tsv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Original: 21.60 | Corrected: 36.92 | Multi: 26.69\n",
            "chrF++ Original: 52.90 | Corrected: 62.70 | Multi: 55.80\n",
            "BERTScore F1 Original: 0.8193 | Corrected: 0.8206\n",
            "\n",
            "📘 Evaluating: DevTest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Baseline not Found for bert-base-multilingual-cased on nso at /usr/local/lib/python3.11/dist-packages/bert_score/rescale_baseline/nso/bert-base-multilingual-cased.tsv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Original: 19.11 | Corrected: 19.11 | Multi: 38.86\n",
            "chrF++ Original: 53.04 | Corrected: 53.56 | Multi: 61.60\n",
            "BERTScore F1 Original: 0.8173 | Corrected: 0.8179\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Baseline not Found for bert-base-multilingual-cased on nso at /usr/local/lib/python3.11/dist-packages/bert_score/rescale_baseline/nso/bert-base-multilingual-cased.tsv\n"
          ]
        }
      ],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF\n",
        "from bert_score import score\n",
        "\n",
        "def evaluate_set(name, model_outputs, source_sentences, original_refs, corrected_refs):\n",
        "    print(f\"📘 Evaluating: {name}\")\n",
        "    bleu = BLEU()\n",
        "    chrf = CHRF()\n",
        "\n",
        "    # Format for multi-reference\n",
        "    multi_refs = [[o, c] for o, c in zip(original_refs, corrected_refs)]\n",
        "\n",
        "    # Metrics\n",
        "    bleu_orig = bleu.corpus_score(model_outputs, [[o] for o in original_refs])\n",
        "    bleu_corr = bleu.corpus_score(model_outputs, [[c] for c in corrected_refs])\n",
        "    bleu_multi = bleu.corpus_score(model_outputs, multi_refs)\n",
        "\n",
        "    chrf_orig = chrf.corpus_score(model_outputs, [[o] for o in original_refs])\n",
        "    chrf_corr = chrf.corpus_score(model_outputs, [[c] for c in corrected_refs])\n",
        "    chrf_multi = chrf.corpus_score(model_outputs, multi_refs)\n",
        "\n",
        "    # BERTScore (corrected only)\n",
        "    _, _, F_corr = score(model_outputs, corrected_refs, lang='nso', rescale_with_baseline=True)\n",
        "    _, _, F_orig = score(model_outputs, original_refs, lang='nso', rescale_with_baseline=True)\n",
        "\n",
        "    # Display\n",
        "    print(f\"BLEU Original: {bleu_orig.score:.2f} | Corrected: {bleu_corr.score:.2f} | Multi: {bleu_multi.score:.2f}\")\n",
        "    print(f\"chrF++ Original: {chrf_orig.score:.2f} | Corrected: {chrf_corr.score:.2f} | Multi: {chrf_multi.score:.2f}\")\n",
        "    print(f\"BERTScore F1 Original: {F_orig.mean().item():.4f} | Corrected: {F_corr.mean().item():.4f}\")\n",
        "    print(\"\")\n",
        "\n",
        "# Then call for both:\n",
        "evaluate_set(\"Dev\", dev_outputs, english_source_sentences_dev, original_sepedi_references_dev, all_corrected_references_by_lang[\"dev\"][\"nso_Latn\"])\n",
        "evaluate_set(\"DevTest\", devtest_outputs, english_source_sentences_devtest, original_sepedi_references_devtest, all_corrected_references_by_lang[\"devtest\"][\"nso_Latn\"])\n",
        "\n",
        "\n",
        "# # --- Evaluate DEV translations ---\n",
        "# if dev_translations: # Only evaluate if translations were generated\n",
        "#     print(f\"\\n--- Evaluating {target_lang} (DEV set) ---\")\n",
        "#     # Get corrected references for the DEV split\n",
        "#     if target_lang in all_corrected_references_by_lang[\"dev\"]:\n",
        "#         raw_references_dev = all_corrected_references_by_lang[\"dev\"][target_lang]\n",
        "#         print(raw_references_dev)\n",
        "#         if raw_references_dev:\n",
        "#             # Structure references as list of lists for sacrebleu\n",
        "#             corrected_references_dev = [[ref] for ref in raw_references_dev]\n",
        "\n",
        "#             # Check for matching lengths before evaluating\n",
        "#             if len(dev_translations) == len(corrected_references_dev):\n",
        "#                 # Calculate BLEU for DEV\n",
        "#                 bleu_score_dev = bleu_metric.corpus_score(dev_translations, corrected_references_dev)\n",
        "#                 print(f\"DEV BLEU Score for {target_lang}: {bleu_score_dev.score}\")\n",
        "\n",
        "#                 # Calculate chrF++ for DEV\n",
        "#                 chrf_score_dev = chrf_metric.corpus_score(dev_translations, corrected_references_dev)\n",
        "#                 print(f\"DEV chrF++ Score for {target_lang}: {chrf_score_dev.score}\")\n",
        "#             else:\n",
        "#                 print(f\"Warning: Mismatched number of generated translations ({len(dev_translations)}) and DEV references ({len(corrected_references_dev)}) for {target_lang}. Skipping DEV evaluation.\")\n",
        "#         else:\n",
        "#             print(f\"Corrected references list is empty for {target_lang} in DEV split. Skipping DEV evaluation.\")\n",
        "#     else:\n",
        "#         print(f\"Corrected references not found for {target_lang} in DEV split dictionary. Skipping DEV evaluation.\")\n",
        "# else:\n",
        "#     print(f\"No translations generated for {target_lang} (DEV set). Skipping DEV evaluation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOKHXClNlmNj"
      },
      "source": [
        "## COMET Setup and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JV2gIqhnlt7m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "4d6839fcc972451ab276b9eec4ebe7ba",
            "2a23217347a94a6b84bde81efebb88d5",
            "87d5584ab72b4d17a179660a4fe7ddd6",
            "5a729ad7481543f4ab469d78313ee389",
            "12ccb6eff76b4e4ab473c658d32c022f",
            "d4099b14ae694c679610294c0f4cc40d",
            "ea5c3f9c966f4b3ab615cab858d23ba3",
            "74772030290e45b2a648c708589fecae",
            "751ec3133a8d45e6a607cabb38d33303",
            "f97341c4b74c4260865d61db165804bc",
            "2fea72b368754ff3af01bba64935496c"
          ]
        },
        "outputId": "c21ded4c-422c-4871-a848-a30128236824"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d6839fcc972451ab276b9eec4ebe7ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
          ]
        }
      ],
      "source": [
        "from comet import download_model, load_from_checkpoint\n",
        "\n",
        "# Download and load pre-trained COMET model\n",
        "model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "comet_model = load_from_checkpoint(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "q5lWTHIHl7rr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "546dbc48-cb46-4123-9e88-caa2f784aec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 COMET Evaluation for Dev\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting DataLoader 0: 100%|██████████| 125/125 [00:37<00:00,  3.35it/s]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dev COMET Score: 0.6663\n",
            "📘 COMET Evaluation for DevTest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting DataLoader 0: 100%|██████████| 127/127 [00:36<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DevTest COMET Score: 0.6680\n"
          ]
        }
      ],
      "source": [
        "def evaluate_comet(srcs, hyps, refs, name=\"Dev\"):\n",
        "    print(f\"📘 COMET Evaluation for {name}\")\n",
        "    data = [{\"src\": s, \"mt\": h, \"ref\": r} for s, h, r in zip(srcs, hyps, refs)]\n",
        "    comet_scores = comet_model.predict(data, batch_size=8, gpus=1 if torch.cuda.is_available() else 0)\n",
        "    print(f\"{name} COMET Score: {comet_scores['system_score']:.4f}\")\n",
        "    return comet_scores\n",
        "\n",
        "comet_scores_dev = evaluate_comet(english_source_sentences_dev, dev_outputs, all_corrected_references_by_lang[\"dev\"][\"nso_Latn\"], name=\"Dev\")\n",
        "comet_scores_devtest = evaluate_comet(english_source_sentences_devtest, devtest_outputs, all_corrected_references_by_lang[\"devtest\"][\"nso_Latn\"], name=\"DevTest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WL3eFbXmJGm"
      },
      "source": [
        "## BLEURT Setup and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "U9jzWm5xmIjr"
      },
      "outputs": [],
      "source": [
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V72-ZJzMnnyk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "a0ee5184-5949-4d98-e5db-a1fccb48b85a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEURT Score Dev (mean): -0.0599\n",
            "Min: -0.7548, Max: 0.7757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEURT Score DevTest (mean): -0.0804\n",
            "Min: -1.0385, Max: 0.6055\n"
          ]
        }
      ],
      "source": [
        "def evaluate_bleurt(model_outputs, corrected_refs, name=\"Dev\"):\n",
        "    # Load BLEURT metric\n",
        "    bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=\"bleurt-base-128\")\n",
        "\n",
        "    # Evaluate\n",
        "    results = bleurt.compute(predictions=model_outputs, references=corrected_refs)\n",
        "    scores = results['scores']\n",
        "    print(f\"BLEURT Score {name} (mean): {sum(scores)/len(scores):.4f}\")\n",
        "    print(f\"Min: {min(scores):.4f}, Max: {max(scores):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "evaluate_bleurt(dev_outputs, all_corrected_references_by_lang[\"dev\"][\"nso_Latn\"], name=\"Dev\")\n",
        "evaluate_bleurt(devtest_outputs, all_corrected_references_by_lang[\"devtest\"][\"nso_Latn\"], name=\"DevTest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BmcPXZxqkBP"
      },
      "source": [
        "## Human evaluation of 5 selected sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9aRCAqdWqjUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b728ce37-d8a2-428a-fd64-b052447e4f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📘 Selected indices for Dev set:\n",
            "High COMET score: 307\n",
            "Median COMET score: 859\n",
            "Low COMET score: 845\n",
            "Random 1: 298\n",
            "Random 2: 281\n",
            "\n",
            "📘 Selected indices for DevTest set:\n",
            "High COMET score: 589\n",
            "Median COMET score: 269\n",
            "Low COMET score: 16\n",
            "Random 1: 658\n",
            "Random 2: 6\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def select_indices(scores, label):\n",
        "    sorted_indices = np.argsort(scores)\n",
        "    high = sorted_indices[-1]\n",
        "    low = sorted_indices[0]\n",
        "    mid = sorted_indices[len(scores) // 2]\n",
        "    rand1, rand2 = np.random.choice(len(scores), size=2, replace=False)\n",
        "    print(f\"\\n📘 Selected indices for {label} set:\")\n",
        "    print(f\"High COMET score: {high}\")\n",
        "    print(f\"Median COMET score: {mid}\")\n",
        "    print(f\"Low COMET score: {low}\")\n",
        "    print(f\"Random 1: {rand1}\")\n",
        "    print(f\"Random 2: {rand2}\")\n",
        "    return [high, mid, low, rand1, rand2]\n",
        "\n",
        "# Run this using your actual COMET score lists\n",
        "dev_indices = select_indices(comet_scores_dev[\"scores\"], \"Dev\")\n",
        "devtest_indices = select_indices(comet_scores_devtest[\"scores\"], \"DevTest\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d6839fcc972451ab276b9eec4ebe7ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a23217347a94a6b84bde81efebb88d5",
              "IPY_MODEL_87d5584ab72b4d17a179660a4fe7ddd6",
              "IPY_MODEL_5a729ad7481543f4ab469d78313ee389"
            ],
            "layout": "IPY_MODEL_12ccb6eff76b4e4ab473c658d32c022f"
          }
        },
        "2a23217347a94a6b84bde81efebb88d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4099b14ae694c679610294c0f4cc40d",
            "placeholder": "​",
            "style": "IPY_MODEL_ea5c3f9c966f4b3ab615cab858d23ba3",
            "value": "Fetching 5 files: 100%"
          }
        },
        "87d5584ab72b4d17a179660a4fe7ddd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74772030290e45b2a648c708589fecae",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_751ec3133a8d45e6a607cabb38d33303",
            "value": 5
          }
        },
        "5a729ad7481543f4ab469d78313ee389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f97341c4b74c4260865d61db165804bc",
            "placeholder": "​",
            "style": "IPY_MODEL_2fea72b368754ff3af01bba64935496c",
            "value": " 5/5 [00:00&lt;00:00, 415.25it/s]"
          }
        },
        "12ccb6eff76b4e4ab473c658d32c022f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4099b14ae694c679610294c0f4cc40d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea5c3f9c966f4b3ab615cab858d23ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74772030290e45b2a648c708589fecae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "751ec3133a8d45e6a607cabb38d33303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f97341c4b74c4260865d61db165804bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fea72b368754ff3af01bba64935496c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}